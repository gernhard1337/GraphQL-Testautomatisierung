\chapter{Auswertung / Experiment / Vergleich mit Property-based Testing}

\section{Vergleichmetriken}
Bevor wir einen tatsächlichen Vergleich beider Methoden durchführen werden erst einmal die Metriken eingeführt in denen
sich Verglichen wird.
Hierdurch wird einfacher verständlich welche Punkte miteinander verglichen werden.
Wir werden einige neue Metriken einführen aber auch Metriken nutzen die in \textit{Property-based Testing}\cite{property-based-testing} genutzt wurden.

\subsection{Metriken aus Property-based Testing}

In \textit{Property-based Testing} wurden zwei Metriken eingeführt, um die Methode zu evaluieren.
Hierbei wurden zwei Forschungsfragen entwickelt.

\begin{enumerate}
    \item Welche Schema Coverage kann mit der Methode erreicht werden? \cite[vgl. RQ1]{property-based-testing}
    \item Wie gut ist die Fehlerfindungskapazität der Methode? \cite[vgl. RG2]{property-based-testing}
\end{enumerate}
\caption{Forschungsfragen aus Property-based Testing}

Zur Auswertung der Methode wurden zwei Testsysteme genutzt.
Das erste Testsystem ist eine eigens entwickelte GraphQL-API die bekannte Fehler besitzt \cite[vgl. A.1]{property-based-testing}.
Testsystem 2 ist GitLab.
Ein häufig genutztes Tool für GitServer mit DevOps Kapazitäten.
Gitlab bietet seine API auch als GraphQL an und durch seine riesige Größe eignet sich GitLab als solides Testsystem. \cite[vgl. A2]{property-based-testing}
Unser entwickelter Prototyp soll in exakt dem gleichen Umfeld seine Tests generieren.
Wir erwarten, dass wir möglichst die selben Fehler finden wie die ursprüngliche Methode und positiv wäre, wenn wir mehr und neue Fehler finden würden.
Beide Forschungsfragen werden im folgenden nocheinmal näher erläutert da diese ein wenig spezialisiert sind und Wissen über Methode
ist wichtig um die Ergebnisse korrekt einordnen zu können.

\subsection{Fehlerfindungskapazitäten}

Mit Fehlerfindungskapazitäten ist gemeint wie zuverlässig die Methode tatsächliche Fehler findet.
Hierfür werden die beiden zuvor benannten APIs getestet und es wird geprüft ob die Methode die Fehler finden konnte.
Um zu verifizieren, dass die Methode möglichst viele Fehler findet gibt es eine Test API die
initial mit bekannten Fehlern versehen wird.
Die \textit{Property-based Methode} hat 11 von 15 Fehlern im speziell vorbereiteten System gefunden.
Bei GitLab wurden 15 bugs gefunden.
Unsere entwickelte Methode soll mindestens die gleichen Fehler finden und idealerweise mehr.

\subsubsection{Schema Coverage / Schema (Ab)Überdeckung}

Dadurch, dass \textit{Property-based Testing} auf zufallsbasierter Testgenerierung basiert stellt sich hier die Frage, wie gut die Methode
die API abdeckt und inwiefern die generierten Tests ausreichend sind.
Dies kommt insbesondere zu tragen, wenn die maximale Pfadlänge ausgehend vom Query Knoten größer ist als die erlaubte Rekursionstiefe des Prototypens.
In Property-based Testing wird definiert, dass die generierten Tests eine Full-Schema Coverage erreichen, wenn gilt:

\begin{definition}
    Für alle Objekte des Schemas: Bilde alle Tupel \{Object, Field \}.
    Ein Schema hat eine ideale Coverage wenn alle Tupel durch einen Test abgedeckt sind.
\end{definition}

Wie in Property-based Testing schon erwähnt: \textit{da keine Coverage Metric für GraphQL Blackbox Test Auswertung exisitiert, starten wir mit einem sehr
einfachen und intuitiven Ansatz}~\cite[vgl. B. Measuring Schema Coverage]{property-based-testing}.
In der Tat ist das vorgestellte Coveragekriterirum ein sehr einfaches Kriterium.
Es lässt zum Beispiel die Beziehungen zwischen allen Knoten aus und beachtet nur, dass alle Knoten inbegriffen sind mit allen Feldern.
Hiermit entspricht das definierte Coverage-Kriterium einer Kombination aus Edge- und Nodecoverage.
Denn alle Knoten müssen abgedeckt sein und alle Kanten ausgehend von den Knoten.
Wie zuvor gesehen ist ein solches Kriterium allerdings noch nicht ausreichend für eine ideale Testabdeckung, da
zum Beispiel die verschiedenen Kantenkombinationen außer acht gelassen werden und sich somit doch noch Fehler im Code befinden können.
Wesentlicher Unterschied beider Methoden ist insbesondere, dass \textit{Property-based Testing} überprüfen muss ob es diese Coverage erreicht.
Unsere vorgestellte Methode stellt sicher, dass diese Coverage erreicht ist bevor sie aufhört mit dem generieren.
Die Überprüfung der Schema-Coverage in Property-based Testing geschah durch ausprobieren.
Hierbei wurde ausprobiert wie viele Testgenerierungen benötigt wurden, um das definierte Kriterium zu erfüllen.
Um ein 100\% Coverage beim GitLab Schema zu erreichen waren verschiedene Anzahlen an Iterationen nötig bei verschiedenen Rekursionslimits.
Eine 100\% Coverage wurde bei GitLab nur erreicht wenn 10000 Tests mit Rekursionslimit 4 erstellt wurden.
Die Berechnungszeit war hierbei 931 Sekunden.
Dies ist zwar der schlechteste Wert in der gesamten Statistik und man könnte meinen, dass der Vergleich
nun nicht genau wäre - allerdings ist dies auch der einzige Wert der verlässlich 100\% Coverage geliefert hat.
Unser Ziel ist es also, weniger als 10.000 Tests und 930 Sekunden zu benötigen um das hier definierte CoverageKriterium zu erfüllen.

\subsection{Neue Metrik}

Näheres betrachten des Codes von \textit{Property-based Testing} offenbarte einen signifikanten Fehler in der Definition der Schema Coverage.
Hierbei ist besonders wichtig zu wissen, wie GraphQL unter der Haube funktioniert.
GraphQL verarbeitet die Schritte einer Query sequentiell.
Nutzen wir die zufällig generierte Query aus \textit{Property based Testing Fig. 9}\cite{property-based-testing}.
Die Query lautet:
\begin{lstlisting}[language=GraphQL]
    {projects(id: "7x8Z"){description members{name}}}
\end{lstlisting}

Stellen wir nun diese Anfrage an die API und es existiert kein $Project$ mit der id 7x8Z so hat die Funktion
einen return Value von $null$.
Ein Return Value von null bedeutet jedoch, dass GraphQL den Pfad nicht weiter auswerten wird und die Funktion des Resolvers hinter dem $members$ Feld nicht ausgeführt wird.
Diese Query würde jedoch die Coverage für die Tupel Project und Members erfüllen aus der vorigen Definition, ohne, dass diese wirklich getestet wurde.
Laut Property-based Testing wird hierdurch angenommen, dass die Query erfolgreich ist wenn die Query erfolgreich ist.
Allerdings haben wir nun ungetesteten Code der als getestet betrachtet wird.
Wir wollen nun eine Metrik einführen die überprüft wieviel der zufällig generierten Querys tatsächlich komplett getestet haben.
Hierfür wird folgende Metrik eingeführt:

\begin{definition}
    Für alle Querys und dazugehörigen Responses wird die Pfadlänge bestimmt.
    Eine erfolgreiche Query hat dann zwei mögliche Szenarien:
        \begin{enumerate}
            \item Pfadlänge(Query) = Pfadlänge(Response)
            \item Pfadlänge(Query) > Pfadlänge(Response)
        \end{enumerate}
\end{definition}

Tritt Fall 1 ein so hat die Query wirklich alle Funktionen getestet.
Tritt Fall 2 ein so hat die Query nicht alle Funktionen getestet.
Zählt man nun alle Querys zusammen kann man auswerten zu wieviel Prozent die gesamte erwartete Pfadlänge tatsächlich ausgeführt wurde indem die Pfadlänge(Response) hinzugezogen wird.

\section{Threats to Validity / Limitierungen}

Bevor wir mit dem eigentlichen Vergleich beginnen muss noch kurz eingeordnet werden inwiefern die Experimente zu betrachten
sind und unter welchen Vorraussetzungen der Vergleich geschieht.

\subsection{Argumentgeneratoren}

Wie in $8.1.3$ angesprochen ist es wichtig, dass GraphQL für jede Funktionen einen Wert ungleich $null$ bekommt, sodass der
Pfad weitergegangen werden kann und die Funktionen in diesem getestet werden.
Um Bedingungen zu begünstigen werden die Argumentgeneratoren für jedes Experiment angepasst, sodass es sehr viel wahrscheinlich ist, dass
die generierten Argumente auch zum SUT passen und die allgemeine Query-Qualität hierdurch besser wird.
Es hängt dann explizit davon ab wie sehr die Argumentgeneratoren angepasst werden denn ein einfaches anpassen hat sich \textit{Property-based Testing}\cite{property-based-testing}
auch erlaubt.
Hierbei sei zum Beispiel erwähnt, dass eine Type $ID$ in GraphQL als String wert definiert ist, häufig in Implementierung jedoch als Zahlenstring genutz wird.
Eine beispielhafte Anpassung wäre hier nun, dass wir den Generator für den Type $ID$ so anpassen, dass er nur Argumente für  $ID$ zurückliefert die ein Zahlenwert sind
in einem gewissen Bereich der durch das SUT abgebildet wird.



